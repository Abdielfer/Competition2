{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math, decimal\n",
    "from math import exp\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\", index_col = None)\n",
    "y = train[['LABELS']]\n",
    "x = train.drop('LABELS', axis=1)\n",
    "xMean = x.mean()\n",
    "test_nolabels = pd.read_csv(\"test_nolabels.csv\", index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62000, 217)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting trainig/Validation\n",
    "x_train, x_validation, y_train, y_validation = train_test_split( x,y, test_size=0.2)\n",
    "## Replacing possible missing values\n",
    "x_train = x_train.fillna(xMean)\n",
    "x_validation = x_validation.fillna(xMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (49600, 217)   : (12400, 217)\n",
      "Label balance on Training set:  \n",
      " 1.0    33030\n",
      "0.0    16570\n",
      "Name: LABELS, dtype: int64\n",
      "Label balance on Validation set:  \n",
      " 1.0    8187\n",
      "0.0    4213\n",
      "Name: LABELS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Data eploration\n",
    "print(\"\",np.shape(x_train),\"  :\",np.shape(x_validation) )\n",
    "# print(y_train['LABELS'].value_counts())\n",
    "\n",
    "print(\"Label balance on Training set: \", \"\\n\", y_train['LABELS'].value_counts())\n",
    "print(\"Label balance on Validation set: \", \"\\n\", y_validation['LABELS'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_train.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To adapt the prediction to Kaggel format\n",
    "def formating_prediction(predictions): \n",
    "        '''\n",
    "        return de predicted classes from the hypotesis function result (sigmoid(W,X))\n",
    "        @hypotesis : matrix of probablilities \n",
    "        '''\n",
    "        y_hat = pd.DataFrame({'S.No' : [],'LABELS' : []}, dtype=np.int8) \n",
    "        for i in range(len(predictions)):\n",
    "            y_hat.loc[i] = [i,predictions[i]]\n",
    "       \n",
    "        return pd.DataFrame(data = y_hat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction based on domain knowledge \n",
    "ndvi_list = {'NDVI_jan','NDVI_feb','NDVI_mar','NDVI_apr','NDVI_may','NDVI_jun','NDVI_jul','NDVI_aug','NDVI_sep','NDVI_oct','NDVI_nov','NDVI_dec'}\n",
    "## Extrae los NDVI y escatter contra las labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapting prediction to Kaggel needs\n",
    "def formating_prediction(predictions): \n",
    "        '''\n",
    "        return de predicted classes from the hypotesis function result (sigmoid(W,X))\n",
    "        @hypotesis : matrix of probablilities \n",
    "        '''\n",
    "        y_hat = pd.DataFrame({'S.No' : [],'LABELS' : []}, dtype=np.int8) \n",
    "        for i in range(len(predictions)):\n",
    "            y_hat.loc[i] = [i,predictions[i]]\n",
    "       \n",
    "        return pd.DataFrame(data = y_hat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBOOST\n",
    "def xgboost(X_train, y_train, X_test):    \n",
    "    # fit model no training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy xgboost: %.2f%%\" % (accuracy * 100.0))\n",
    "    predictionsDF = formating_prediction(predictions)\n",
    "    return model, accuracy, predictionsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grill for RF training\n",
    "rf_training_gril = {\n",
    "    'n_estimators': np.linspace(10, 200).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, rf_training_gril, n_jobs = -1, \n",
    "                        scoring = 'roc_auc', cv = 3, \n",
    "                        n_iter = 10, verbose = 1, random_state=RSEED)\n",
    "\n",
    "# Random Forest\n",
    "def randomForest(x_train, y_train, x_validation, y_validation, estimators):\n",
    "    '''\n",
    "    Performe random forest, in the simples way, from Skl library\n",
    "    @estimators: NUmber of estimators\n",
    "    return: model \"classifier_LR\" and a prediction over a x_validation\n",
    "    '''\n",
    "    rf_classifier = RandomForestClassifier(n_estimators = estimators, criterion = 'entropy', random_state = 42)\n",
    "    rf_classifier.fit(x_train, y_train)\n",
    "\n",
    "    # Predicting the over the validation set\n",
    "    y_pred = rf_classifier.predict(x_validation)\n",
    "    print(error(y_pred,y_validation))\n",
    "\n",
    "    prediction = rf_classifier.predict(test_Set)\n",
    "    df = pd.DataFrame(prediction)\n",
    "    df.to_csv(\"rf_prediction.csv\", index = None)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Uncomment to save the model\n",
    "    # joblib.dump(rf_classifier, 'randomforestmodel.pkl')\n",
    "    \n",
    "    predictionsDF = formating_prediction(prediction)\n",
    "    return rf_classifier, accuracy, predictionsDF \n",
    "\n",
    "        # # load the model from disk to predict new dataSet\n",
    "        # loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        # result = loaded_model.score(X_test, Y_test)\n",
    "\n",
    "def evaluate_Rf(X_train, y_train, X_test, y_test, rf_classifier):\n",
    "    features = X_train.columns\n",
    "    train_rf_predictions = rf_classifier.predict(X_train)\n",
    "    train_rf_probs = rf_classifier.predict_proba(X_train)[:, 1]\n",
    "\n",
    "    rf_predictions = rf_classifier.predict(X_test)\n",
    "    rf_probs = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs)\n",
    "    cm = confusion_matrix(y_test, rf_predictions)\n",
    "    plot_confusion_matrix(cm, classes = ['Cropland', 'Non-Cropland'],\n",
    "                      title = 'Cropland Confusion Matrix')\n",
    "    fi_model = pd.DataFrame({'feature': features,\n",
    "                   'importance': rf_classifier.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = True)\n",
    "     \n",
    "    return fi_model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and format prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and sve predisction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eedf6d570df146484a47fd0eae676e78ed396670004acbd373a3bc98c7d3284"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ift6758-conda-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
