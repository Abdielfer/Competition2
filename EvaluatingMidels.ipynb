{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math, decimal\n",
    "from math import exp\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV,GridSearchCV \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,roc_curve, auc, roc_auc_score, f1_score\n",
    "from sklearn.calibration import calibration_curve, CalibrationDisplay\n",
    "from xgboost import XGBClassifier,XGBRFClassifier\n",
    "from collections import Counter\n",
    "seedRF =50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "                ## uncoment to apply\n",
    "####    importing original datasets  \n",
    "# train = pd.read_csv(\"train.csv\", index_col = None)\n",
    "# y = train[['LABELS']]\n",
    "# x = pd.read_csv(\"x_standardized.csv\", index_col = None)\n",
    "# # x = train.drop('LABELS', axis=1)\n",
    "# ## Correction of empty values with means by columns\n",
    "# xMean = x.mean()\n",
    "# x = x.fillna(xMean)\n",
    "# test_nolabels = pd.read_csv(\"test_nolabels.csv\", index_col = None)\n",
    "# test_nolabels = test_nolabels.fillna(xMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "                ## uncoment to apply   \n",
    "#### its all done in x_estandardized.csv and nolabels_standardized.csv datasets\n",
    "# toposElevation = {\"topo_elevation_jan\",'topo_elevation_feb','topo_elevation_mar','topo_elevation_apr','topo_elevation_may','topo_elevation_jun','topo_elevation_jul','topo_elevation_aug','topo_elevation_sep','topo_elevation_oct','topo_elevation_nov','topo_elevation_dec'}\n",
    "# topoSlope = {'topo_slope_jan','topo_slope_feb','topo_slope_mar','topo_slope_apr','topo_slope_may','topo_slope_aug','topo_slope_jun','topo_slope_jul','topo_slope_sep','topo_slope_oct','topo_slope_nov','topo_slope_dec'}\n",
    "# topoElevationDF = x[toposElevation]\n",
    "# topoElevationMean = topoElevationDF.mean(axis=1)\n",
    "# x = x.drop(toposElevation,axis=1)\n",
    "# x['topoElevationMean'] = topoElevationMean\n",
    "# topoSlopeDF = x[topoSlope]\n",
    "# topoSlopeMean = topoSlopeDF.mean(axis=1)\n",
    "# x = x.drop(topoSlope, axis=1)\n",
    "# x['topoSlope'] = topoSlopeMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data manipulation and helper methods  ###\n",
    "def meanStd(dataset):\n",
    "    '''\n",
    "    dataset_minmax(dataset)\n",
    "    return a list like {min:#,max:#}\n",
    "    # Find the min and std values for each column\n",
    "    '''\n",
    "    col = dataset.shape[1]\n",
    "    meanVal, stdVal = 0,0\n",
    "    stats = list()\n",
    "    for i in range(col):\n",
    "        val = dataset.iloc[:, i]\n",
    "        meanVal = np.mean(val)\n",
    "        stdVal = np.std(val)\n",
    "        stats.append([meanVal,stdVal])\n",
    "    return stats\n",
    "\n",
    "def standardize_data(dataset, mean_std):\n",
    "    '''\n",
    "    standardize_data(dataset, mean_std)\n",
    "    @mean_std: @arguent: list of min/max valuer per column {min:#,max:#}\n",
    "    # Rescale dataset columns to the range 0-1\n",
    "    '''\n",
    "    col = dataset.shape[1]\n",
    "    row = dataset.shape[0]\n",
    "    for i in range(1,col):\n",
    "        for n in range(row):\n",
    "            dataset.iloc[n,i] -= mean_std[i][0]\n",
    "            dataset.iloc[n,i] /= mean_std[i][1]\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "     ## uncoment to apply\n",
    "# #preprocessing No_label datase:\n",
    "# test_nolabels = test_nolabels.fillna(xMean)\n",
    "# test_nolabels = test_nolabels.drop(toposElevation, axis=1)\n",
    "# test_nolabels = test_nolabels.drop(topoSlope, axis=1)\n",
    "# test_nolabels['topoElevationMean'] = topoElevationMean\n",
    "# test_nolabels['topoSlope'] = topoSlopeMean\n",
    "# ## standardizing test_nolabels\n",
    "# mean_std_x_noLabel = meanStd(test_nolabels)\n",
    "# test_nolabels = standardize_data(test_nolabels, mean_std_x_noLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### working with standardized dataSets\n",
    "train = pd.read_csv(\"train.csv\", index_col = None)\n",
    "y = train[['LABELS']]\n",
    "x = pd.read_csv(\"x_standardized.csv\", index_col = None)\n",
    "# test_nolabels = pd.read_csv(\"nolabels_standardized.csv\", index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validation, y_train, y_validation = train_test_split( x,y, test_size=0.2)\n",
    "y_train = (np.array(y_train).astype('int')).ravel()\n",
    "y_validation = (np.array(y_validation).astype('int')).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data shape exploration\n",
    "print(\"\",np.shape(x_train),\"  :\",np.shape(x_validation) )\n",
    "print(\"Label balance on Training set: \", \"\\n\", y_train['LABELS'].value_counts())\n",
    "print(\"Label balance on Validation set: \", \"\\n\", y_validation['LABELS'].value_counts())\n",
    "print(\"Label balance on Validation set: \", \"\\n\", y_validation['LABELS'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some helpers function\n",
    "# To adapt the prediction to Kaggel format of submission \n",
    "def formating_prediction(predictions): \n",
    "        '''\n",
    "        Return the prediction in the Kaggle format submition\n",
    "        @hypotesis : column of classes \n",
    "        '''\n",
    "        y_hat = pd.DataFrame({'S.No' : [],'LABELS' : []}, dtype=np.int8) \n",
    "        for i in range(len(predictions)):\n",
    "            y_hat.loc[i] = [i,predictions[i]]\n",
    "        return pd.DataFrame(data = y_hat) \n",
    "\n",
    "def predictOnSet(model, x_test):\n",
    "    prediction = model.predict(x_test)\n",
    "    return prediction\n",
    "\n",
    "def savingModels(classifier, modelFileName):\n",
    "    '''\n",
    "    NOTE: Do not forget the extention = *.pkl\n",
    "    Save as : 'modelFileName.pkl'\n",
    "    '''\n",
    "    joblib.dump(classifier, modelFileName)\n",
    "\n",
    "\n",
    "def importModel(modefname):\n",
    "    model = joblib.load(modefname)\n",
    "    return model\n",
    "\n",
    "def savePrediction(prediction, filename):\n",
    "    '''\n",
    "    Save predictions\n",
    "    @argument: filename: Remenber EXTENTION 'filename.csv'\n",
    "    '''\n",
    "    prediction = prediction.astype('int32') #exsure prediction as integer\n",
    "    predictions_DF = formating_prediction(prediction)\n",
    "    return predictions_DF.to_csv(filename, index = None)\n",
    "\n",
    "## modle evaluation\n",
    "def metric_RocAuc(y_probability, y_validation, estimator_name):\n",
    "    '''\n",
    "    Calculate and plt ROC metric\n",
    "    @argument: y_probability : the probability class=1.\n",
    "    @argument: y_validation: True labels.\n",
    "    fpr, tpr = false_positive, true_positive.\n",
    "    Return: \"false_positive\" and \"true_positive\", ROC_auc metric.\n",
    "    '''\n",
    "    fpr, tpr, _ = roc_curve(y_validation, y_probability) \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig, axes = plt.subplots(constrained_layout=True,figsize=(5,3), dpi=150)\n",
    "    fig.suptitle(estimator_name)\n",
    "    axes.plot([0, 1], [0, 1], color= 'k',linestyle=\"--\") # perfect fit \n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                       estimator_name=estimator_name)\n",
    "    display.plot(ax=axes)\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "## Show some evaluation criteria on the clasifier\n",
    "def evaluate_model(x_train, y_train, x_validation, y_validation, classifier):\n",
    "    features = x_train.columns\n",
    "    validation_Prediction = classifier.predict(x_validation)\n",
    "    validation_PredictedProb = classifier.predict_proba(x_validation)[:, 1]\n",
    "    ### ROC metric and curve #####\n",
    "    clasifierName = type(classifier).__name__\n",
    "    metric_RocAuc(validation_PredictedProb, y_validation,clasifierName)\n",
    "    fi_model = pd.DataFrame({'feature': features,\n",
    "                   'importance': classifier.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "    clasifierNameExtended = clasifierName + \"_info_fi\"     \n",
    "    fi_model.to_csv(clasifierNameExtended, index = None)\n",
    "    return fi_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Ramdomized RF  ####\n",
    "# Hyperparameter grid NOTE: Grid search was transformed several time to enlage the exploration. \n",
    "# Best ramdom seach grid has been publiched in the Report\n",
    "param_grid = {\n",
    "    'n_estimators': np.linspace(10, 400,20).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "# Estimator for use in random search\n",
    "estimator = RandomForestClassifier(random_state = seedRF)\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = 'roc_auc', cv = 3, \n",
    "                        n_iter = 10, verbose = 1, random_state=seedRF)\n",
    "# Random searsh  \n",
    "y_train = np.array(y_train).ravel()\n",
    "rs.fit(x_train, y_train)\n",
    "print(rs.best_params_, \"\\n\")\n",
    "### Working with best estimator from RandomizedSearch \n",
    "best_model = rs.best_estimator_\n",
    "savingModels(best_model, \"rf_RandomSearch.pkl\")\n",
    "## Evaluating ROC Curve and extracting features priority\n",
    "fi_model = evaluate_model(x_train, y_train, x_validation, y_validation, test_nolabels_prediction)\n",
    "## Predicting on No_Labeled dataset and saving prediction ready to submit\n",
    "# test_nolabels_prediction = predictOnSet(best_model, test_nolabels)\n",
    "# savePrediction(test_nolabels_prediction, 'first_rfSearch_noLabelPrediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBClassifier\n",
    "\n",
    "estimator = XGBClassifier(use_label_encoder=False,subsample=0.9, colsample_bynode=0.2)\n",
    "param_grid = {\n",
    "    'n_estimators': np.linspace(50, 500, 10).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3,20,5).astype(int)),\n",
    "    'max_features': [None] + list(np.arange(0.2, 0.9, 0.1)),\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = 'roc_auc', cv = 3, \n",
    "                        n_iter = 4, verbose = 1, random_state=47)\n",
    "\n",
    "rs.fit(x_train, y_train)\n",
    "print(rs.best_params_, \"\\n\")\n",
    "model = rs.best_estimator_\n",
    "\n",
    "savingModels(model, \"XGBClassifierGridSearh.pkl\")\n",
    "\n",
    "# To monitor evolution on mode.fit() and find the early soping point\n",
    "eval_set = [(x_validation, y_validation)]\n",
    "model.fit(x_train, y_train, early_stopping_rounds=10, eval_metric=\"error\", eval_set=eval_set, verbose=True)\n",
    "\n",
    "savingModels(model, \"XGBClassifierGridSearh.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ploting the model behavior on train and validation datasets over epochs\n",
    "eval_set = [(x_train, y_train),(x_validation, y_validation)]\n",
    "model.fit(x_train, y_train, early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\n",
    "results = model.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, epochs)\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'], label='Validation')\n",
    "ax.legend()\n",
    "pyplot.ylabel('log_loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "pyplot.show()\n",
    "# plot classification error\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['error'], label='Validation')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Classification Error')\n",
    "pyplot.title('XGBoost Classification Error')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC metric \n",
    "evaluate_model(x_train, y_train, x_validation, y_validation, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_nolabels_prediction = predictOnSet(model, test_nolabels)\n",
    "savePrediction(test_nolabels_prediction, 'XGBoost_EarlyStop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating F1 metrics for all produced models\n",
    "modelNameList =  list{}\n",
    "\n",
    "for model in modelNameList:\n",
    "    model = pd.(model, index_col = None)\n",
    "    model.fit(x_train, y_train) \n",
    "    y_hat = model.predict(x_validation)\n",
    "    f1 = f1_score(model, x_validation,y_validation, average ='binary',zero_division = 0)\n",
    "    roc = roc_auc_score(y_validation, y_hat)\n",
    "    print(\"quality of prediction of \" + model + \", f1_score:\", f1, \", ROC: \", roc,\"\\n\",)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
