{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV,GridSearchCV \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\", index_col = None)\n",
    "y = train[['LABELS']]\n",
    "x = train.drop('LABELS', axis=1)\n",
    "xMean = x.mean()\n",
    "x = x.fillna(xMean)\n",
    "test_nolabels = pd.read_csv(\"test_nolabels.csv\", index_col = None)\n",
    "test_nolabels = test_nolabels.fillna(xMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_list = {'NDVI_jan','NDVI_feb','NDVI_mar','NDVI_apr','NDVI_may','NDVI_jun','NDVI_jul','NDVI_aug','NDVI_sep','NDVI_oct','NDVI_nov','NDVI_dec'}\n",
    "ndviDF = x[ndvi_list]\n",
    "# ndviDF['LABELS'] = y['LABELS']\n",
    "print(ndviDF.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "toposElevation = {\"topo_elevation_jan\",'topo_elevation_feb','topo_elevation_mar','topo_elevation_apr','topo_elevation_may','topo_elevation_jun','topo_elevation_jul','topo_elevation_aug','topo_elevation_sep','topo_elevation_oct','topo_elevation_nov','topo_elevation_dec'}\n",
    "topoSlope = {'topo_slope_jan','topo_slope_feb','topo_slope_mar','topo_slope_apr','topo_slope_may','topo_slope_aug','topo_slope_jun','topo_slope_jul','topo_slope_sep','topo_slope_oct','topo_slope_nov','topo_slope_dec'}\n",
    "topoElevationDF = x[toposElevation]\n",
    "topoElevationMean = topoElevationDF.mean(axis=1)\n",
    "x = x.drop(toposElevation,axis=1)\n",
    "x['topoElevationMean'] = topoElevationMean\n",
    "topoSlopeDF = x[topoSlope]\n",
    "topoSlopeMean = topoSlopeDF.mean(axis=1)\n",
    "x = x.drop(topoSlope, axis=1)\n",
    "x['topoSlope'] = topoSlopeMean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing No_label datase:\n",
    "test_nolabels = test_nolabels.fillna(xMean)\n",
    "test_nolabels = test_nolabels.drop(toposElevation, axis=1)\n",
    "test_nolabels = test_nolabels.drop(topoSlope, axis=1)\n",
    "test_nolabels['topoElevationMean'] = topoElevationMean\n",
    "test_nolabels['topoSlope'] = topoSlopeMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> /n <class 'numpy.ndarray'> /n <class 'pandas.core.frame.DataFrame'> /n <class 'numpy.ndarray'> /n\n"
     ]
    }
   ],
   "source": [
    "x_train, x_validation, y_train, y_validation = train_test_split( x,y, test_size=0.2)\n",
    "y_train = (np.array(y_train).astype('int')).ravel()\n",
    "y_validation = (np.array(y_validation).astype('int')).ravel()\n",
    "\n",
    "print(type(x_train),\"/n\", type(y_train),\"/n\", type(x_validation),\"/n\", type(y_validation),\"/n\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some helpers function\n",
    "# To adapt the prediction to Kaggel format of submission \n",
    "def formating_prediction(predictions): \n",
    "        '''\n",
    "        return de predicted classes from the hypotesis function result (sigmoid(W,X))\n",
    "        @hypotesis : matrix of probablilities \n",
    "        '''\n",
    "        y_hat = pd.DataFrame({'S.No' : [],'LABELS' : []}, dtype=np.int8) \n",
    "        for i in range(len(predictions)):\n",
    "            y_hat.loc[i] = [i,predictions[i]]\n",
    "        return pd.DataFrame(data = y_hat) \n",
    "\n",
    "def predictOnSet(model, x_test):\n",
    "    prediction = model.predict(x_test)\n",
    "    return prediction\n",
    "\n",
    "def savingModels(classifier, modelFileName):\n",
    "    '''\n",
    "    NOTE: Do not forget the extention = *.pkl\n",
    "    Save as : 'modelFileName.pkl'\n",
    "    '''\n",
    "    joblib.dump(classifier, modelFileName)\n",
    "\n",
    "\n",
    "def importModel(modefname):\n",
    "    model = joblib.load(modefname)\n",
    "    return model\n",
    "\n",
    "def savePrediction(prediction, filename):\n",
    "    '''\n",
    "    Save predictions\n",
    "    @argument: filename: Remenber EXTENTION 'filename.csv'\n",
    "    '''\n",
    "    prediction = prediction.astype('int32') #exsure prediction as integer\n",
    "    predictions_DF = formating_prediction(prediction)\n",
    "    return predictions_DF.to_csv(filename, index = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modle evaluation\n",
    "def metric_RocAuc(y_probability, y_validation, estimator_name):\n",
    "    '''\n",
    "    Calculate and plt ROC metric\n",
    "    @argument: y_probability : the probability class=1.\n",
    "    @argument: y_validation: True labels.\n",
    "    fpr, tpr = false_positive, true_positive.\n",
    "    Return: \"false_positive\" and \"true_positive\", ROC_auc metric.\n",
    "    '''\n",
    "    fpr, tpr, _ = roc_curve(y_validation, y_probability) \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig, axes = plt.subplots(constrained_layout=True,figsize=(5,3), dpi=150)\n",
    "    fig.suptitle(estimator_name)\n",
    "    axes.plot([0, 1], [0, 1], color= 'k',linestyle=\"--\") # perfect fit \n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                       estimator_name=estimator_name)\n",
    "    display.plot(ax=axes)\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "## Show some evaluation criteria on the clasifier\n",
    "def evaluate_model(x_train, y_train, x_validation, y_validation, classifier):\n",
    "    features = x_train.columns\n",
    "    validation_Prediction = classifier.predict(x_validation)\n",
    "    validation_PredictedProb = classifier.predict_proba(x_validation)[:, 1]\n",
    "    ### ROC metric and curve #####\n",
    "    clasifierName = type(classifier).__name__\n",
    "    metric_RocAuc(validation_PredictedProb, y_validation,clasifierName)\n",
    "    fi_model = pd.DataFrame({'feature': features,\n",
    "                   'importance': classifier.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "    clasifierNameExtended = clasifierName + \"_info_fi\"     \n",
    "    fi_model.to_csv(clasifierNameExtended, index = None)\n",
    "    return fi_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': np.linspace(10,1000,20).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3,20,8).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.2, 1, 0.1)),\n",
    "    'max_leaf_nodes': [None] + list(np.linspace(5, 50,10).astype(int)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = RandomForestClassifier(random_state = seedRF)\n",
    "\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = 'roc_auc', cv = 3, \n",
    "                        n_iter = 10, verbose = 1, random_state=seedRF)\n",
    "rs.fit(x_train, y_train)\n",
    "print(rs.best_params_, \"\\n\")\n",
    "best_model = rs.best_estimator_\n",
    "savingModels(best_model, \"rf_RandomDSEtReduced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = joblib.load(\"rf_RandomSearch.pkl\")\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators = 300, n_jobs = -1, criterion = 'entropy',max_depth=10,max_features=0.2,max_leaf_nodes=20,max_samples=0.7,bootstrap =True)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_validation)\n",
    "accuracy = accuracy_score(y_validation, y_pred)\n",
    "# recording[str(i)] = accuracy\n",
    "print(str(i) + \"_Accuracy: %.2f%%\" % (accuracy))\n",
    "evaluate_model(x_train, y_train, x_validation, y_validation, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.get_params(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    x_train, x_validation, y_train, y_validation = train_test_split( x,y, test_size=0.2)\n",
    "    y_train = (np.array(y_train).astype('int')).ravel()\n",
    "    y_validation = (np.array(y_validation).astype('int')).ravel()\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_pred = classifier.predict(x_validation)\n",
    "    accuracy = accuracy_score(y_validation, y_pred)\n",
    "    # recording[str(i)] = accuracy\n",
    "    print(str(i) + \"_Accuracy: %.2f%%\" % (accuracy))\n",
    "    evaluate_model(x_train, y_train, x_validation, y_validation, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross validation:\n",
    "kf = model_selection.KFold(n_splits=3, random_state=None) \n",
    "result = cross_val_score(classifier , x_validation, y_validation, cv = kf)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savingModels(classifier, \"RF_400AllRandom.pkl\")\n",
    "\n",
    "test_nolabels_prediction = predictOnSet(classifier, test_nolabels)\n",
    "savePrediction(test_nolabels_prediction, 'RF_400AllRandom.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100, 500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "grid_result = grid_search.fit(X, y)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model,x_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100, 500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "grid_result = grid_search.fit(x_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "model = grid_result.best_estimator_\n",
    "joblib.dump(model, 'AdaBoostGrid.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "model = grid_result.best_estimator_\n",
    "model.fit(x_train, y_train, cla)\n",
    "\n",
    "joblib.dump(model, 'AdaBoostGrid.pkl')\n",
    "evaluate_model(x_train, y_train, x_validation, y_validation, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "evaluate_model(x_train, y_train, x_validation, y_validation, model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
